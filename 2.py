# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zv0J6vM4GdmrTDNtezcjpBGnXuvh43Wb
"""

import streamlit as st
import os
import google.generativeai as genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# --- 1. API 키 설정 및 데이터베이스 준비 ---

# ChromaDB 저장 경로
DB_DIR = "./chroma_db"

# Streamlit Secrets에서 API 키를 가져옵니다.
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=GOOGLE_API_KEY)
except (KeyError, FileNotFoundError):
    st.error("오류: Gemini API 키가 설정되지 않았습니다.")
    st.info("Streamlit 앱의 'Settings > Secrets'에서 GOOGLE_API_KEY를 추가해주세요.")
    st.stop()


def setup_database():
    """
    ChromaDB가 존재하는지 확인하고, 없으면 새로 생성하는 함수.
    Streamlit Cloud의 임시 저장소에 DB를 구축합니다.
    """
    if os.path.exists(DB_DIR):
        st.sidebar.info("기존 데이터베이스를 로드했습니다.")
        return

    st.sidebar.info("새로운 데이터베이스를 구축하고 있습니다. 잠시만 기다려주세요...")

    # 1. 문서 로드
    docs = []
    data_path = "./data"
    pdf_files = [f for f in os.listdir(data_path) if f.endswith('.pdf')]
    if not pdf_files:
        st.error("오류: data 폴더에 PDF 파일이 없습니다.")
        st.stop()

    for pdf_file in pdf_files:
        loader = PyPDFLoader(os.path.join(data_path, pdf_file))
        docs.extend(loader.load())

    # 2. 텍스트 분할
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_documents(docs)

    # 3. 임베딩 및 ChromaDB 저장
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory=DB_DIR
        )
        st.sidebar.success("데이터베이스 구축이 완료되었습니다!")
    except Exception as e:
        st.error(f"데이터베이스 구축 중 오류가 발생했습니다: {e}")
        st.stop()

@st.cache_resource
def get_rag_chain():
    """
    RAG 체인을 초기화하고 반환하는 함수.
    Streamlit의 캐싱을 사용하여 리소스를 효율적으로 관리합니다.
    """
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

        vector_store = Chroma(
            persist_directory=DB_DIR,
            embedding_function=embeddings
        )
        retriever = vector_store.as_retriever()

        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)

        template = """당신은 대한민국 정부 정책 전문가입니다. 사용자의 질문에 대해 아래의 '문서 내용'을 바탕으로, 명확하고 친절하게 답변해주세요.
        문서 내용에 없는 정보는 답변에 포함하지 마세요. 답변은 항상 한국어로 작성해주세요.

        [문서 내용]
        {context}

        [사용자 질문]
        {question}

        [답변]
        """

        prompt = PromptTemplate.from_template(template)

        rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        return rag_chain, retriever
    except Exception as e:
        st.error(f"RAG 파이프라인 초기화 중 오류가 발생했습니다. 오류: {e}")
        st.stop()

# --- 2. Streamlit UI 구성 ---

st.set_page_config(page_title="정책 큐레이터 (Gemini)", page_icon="🤖")
st.title("🤖 정책 큐레이터 (Gemini Ver.)")
st.caption("AI가 정부 정책 문서를 분석하여 질문에 답변해 드립니다.")

with st.sidebar:
    st.header("안내")
    st.markdown("""
    이 앱은 Google Gemini API와 RAG 기술을 사용하여 PDF 문서의 내용을 기반으로 질문에 답변합니다.

    **배포 안내:**
    1. 이 프로젝트를 GitHub에 업로드하세요.
    2. Streamlit Cloud에 로그인하여 새 앱을 만듭니다.
    3. GitHub 리포지토리를 연결합니다.
    4. **Advanced settings...** 에서 `GOOGLE_API_KEY`를 Secrets에 추가합니다.
       - 형식: `GOOGLE_API_KEY = "AIzaSy..."`
    5. **Deploy!** 버튼을 누르면 배포가 시작됩니다.
    """)

# 데이터베이스 설정 실행
setup_database()

# RAG 체인과 retriever 로드
rag_chain, retriever = get_rag_chain()

# 세션 상태를 사용하여 대화 기록 저장
if "messages" not in st.session_state:
    st.session_state.messages = []

# 이전 대화 기록 표시
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message and message["sources"]:
            with st.expander("📚 근거 자료 확인하기"):
                for i, source in enumerate(message["sources"]):
                    st.markdown(f"**문서 {i+1} (출처: {source.metadata.get('source', 'N/A')}, 페이지: {source.metadata.get('page', 'N/A')})**")
                    st.markdown(f"> {source.page_content}")
                    st.markdown("---")

# 사용자 입력 처리
if prompt := st.chat_input("궁금한 정책에 대해 질문해보세요."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.spinner("답변을 생성하는 중입니다..."):
        try:
            answer = rag_chain.invoke(prompt)
            sources = retriever.invoke(prompt)
            bot_message = {"role": "assistant", "content": answer, "sources": sources}
            st.session_state.messages.append(bot_message)
            st.rerun()
        except Exception as e:
            st.error(f"답변 생성 중 오류가 발생했습니다: {e}")