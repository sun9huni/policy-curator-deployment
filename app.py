import streamlit as st
import os
import time
import re
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Qdrant
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableConfig
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_huggingface import HuggingFaceEmbeddings
from sentence_transformers import CrossEncoder

# -----------------------
# PAGE CONFIG
# -----------------------
st.set_page_config(
    page_title="ì •ì±… íë ˆì´í„° v3",
    page_icon="ğŸ¯",
    layout="wide",
)

# -----------------------
# SECRETS
# -----------------------
try:
    openai_api_key = st.secrets["OPENAI_API_KEY"]
except Exception:
    st.error("ì˜¤ë¥˜: OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Streamlit Cloudì˜ Secrets ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# -----------------------
# CSS
# -----------------------
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+KR:wght@300;400;500;600;700&display=swap');
html, body, [class*="css"]  { font-family: 'IBM Plex Sans KR', sans-serif; }
.stApp { background-color: #F0F2F6; }
[data-testid="stSidebar"] { background-color: #FFFFFF; border-right: 1px solid #E0E0E0; }
.stButton > button {
    border: 1px solid #E0E0E0; border-radius: 10px; color: #31333F;
    background-color: #FFFFFF; transition: all 0.2s ease-in-out;
    box-shadow: 0 1px 3px rgba(0,0,0,0.05);
}
.stButton > button:hover {
    border-color: #0068C9; color: #0068C9;
    box-shadow: 0 2px 5px rgba(0,0,0,0.1);
}
div[data-testid="stSidebar"] .stButton > button {
    background-color: #0068C9; color: white; border: none;
}
div[data-testid="stSidebar"] .stButton > button:hover {
    background-color: #0055A3; color: white;
}
</style>
""", unsafe_allow_html=True)

# -----------------------
# RAG COMPONENTS
# -----------------------

DATA_PATH = "./data"

# --- 1. ëª¨ë¸ ë° í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
@st.cache_resource
def get_models_and_prompts():
    """LLM, Reranker, Prompts ë“± ëª¨ë¸ ê´€ë ¨ ê°ì²´ë“¤ì„ ë¡œë“œí•˜ê³  ìºì‹±í•©ë‹ˆë‹¤."""
    llm = ChatOpenAI(api_key=openai_api_key, model="gpt-4o", temperature=0.1)
    reranker_model = CrossEncoder('bongsoo/kpf-cross-encoder-v1')
    QUERY_PROMPT = PromptTemplate(
        input_variables=["question"],
        template="""ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ìœ ì‚¬í•œ ì§ˆë¬¸ë“¤ë¡œ ë‹¤ì‹œ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.
        ìƒì„±ëœ ì§ˆë¬¸ë“¤ì€ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ 3ê°œì˜ ë‹¤ë¥¸ ë²„ì „ì˜ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
        ì§ˆë¬¸ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ê° ì§ˆë¬¸ì€ ë‹¤ìŒ ì¤„ë¡œ êµ¬ë¶„í•´ì£¼ì„¸ìš”.
        ì›ë³¸ ì§ˆë¬¸: {question}""",
    )
    response_prompt = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì •ë¶€ì˜ ì²­ë…„ ì •ì±… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ 'ë¬¸ì„œ ë‚´ìš©'ì„ ë°”íƒ•ìœ¼ë¡œ, ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¨ê³„ë³„ë¡œ ì•Œê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        ë§Œì•½ 'ë¬¸ì„œ ë‚´ìš©'ì— ì§ˆë¬¸ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì •ë³´ê°€ ì—†ë‹¤ë©´, ê·¸ ì‚¬ì‹¤ì„ ë¨¼ì € ì–¸ê¸‰í•œ í›„, ê´€ë ¨ì„±ì´ ë§¤ìš° ë†’ì€ ì •ë³´ê°€ ìˆë‹¤ë©´ ê·¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ ì—°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        ë¬¸ì„œ ë‚´ìš©ì— ì „í˜€ ê´€ë ¨ ì—†ëŠ” ì •ë³´ë§Œ ìˆë‹¤ë©´, "ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.

        [ë¬¸ì„œ ë‚´ìš©]
        {context}

        [ì‚¬ìš©ì ì§ˆë¬¸]
        {question}

        [ë‹µë³€]
        """
    )
    return llm, reranker_model, QUERY_PROMPT, response_prompt

# --- 2. ë²¡í„° ì €ì¥ì†Œ ìƒì„± (âœ¨ 1ë‹¨ê³„ ì ìš©) ---
@st.cache_resource
def create_vector_store():
    """PDF ë¬¸ì„œë¥¼ ë¡œë“œ, ë¶„í• í•˜ê³  'ì •ì±… ìœ í˜•' ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if not os.path.exists(DATA_PATH) or not any(f.endswith('.pdf') for f in os.listdir(DATA_PATH)):
        st.error(f"ì˜¤ë¥˜: '{DATA_PATH}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    with st.spinner("ì •ì±… ë¬¸ì„œë¥¼ ì½ê³  ë¶„ì„í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        documents = []
        for file in os.listdir(DATA_PATH):
            if file.endswith('.pdf'):
                loader = PyPDFLoader(os.path.join(DATA_PATH, file))
                documents.extend(loader.load())

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        chunks = text_splitter.split_documents(documents)

        # âœ¨ [1ë‹¨ê³„] ê° chunkì— 'ì •ì±… ìœ í˜•' ë©”íƒ€ë°ì´í„° ì¶”ê°€
        current_policy_types = []
        for chunk in chunks:
            # ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ 'ì •ì±… ìœ í˜•: ...' íŒ¨í„´ì„ ì°¾ìŠµë‹ˆë‹¤.
            match = re.search(r"ì •ì±… ìœ í˜•:\s*(.*)", chunk.page_content)
            if match:
                # ìœ í˜•ì´ ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ(ì˜ˆ: ë³µì§€/ë¬¸í™”, ê¸ˆìœµ/ìì‚°), ì‰¼í‘œë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
                types_str = match.group(1).strip()
                current_policy_types = [t.strip() for t in types_str.split(',')]

            # chunkì˜ ë©”íƒ€ë°ì´í„°ì— 'policy_type' ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
            chunk.metadata['policy_type'] = current_policy_types

        embedding_model = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sbert-nli",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        vectorstore = Qdrant.from_documents(
            chunks, embedding_model, location=":memory:", collection_name="policy_documents_filtered"
        )
    return vectorstore

# --- 3. RAG ì²´ì¸ êµ¬ì„± (âœ¨ 2ë‹¨ê³„ ì ìš©) ---
def setup_rag_chain(vectorstore, reranker, llm, response_prompt, query_prompt):
    """ì‚¬ìš©ì ê´€ì‹¬ì‚¬ì— ë”°ë¼ ë™ì ìœ¼ë¡œ í•„í„°ë§ë˜ëŠ” RAG ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤."""

    def rerank_documents(inputs):
        query = inputs['question']
        retrieved_docs = inputs['documents']
        unique_docs = list({doc.page_content: doc for doc in retrieved_docs}.values())
        if not unique_docs:
            return []
        pairs = [[query, doc.page_content] for doc in unique_docs]
        scores = reranker.predict(pairs)
        doc_scores = sorted(zip(scores, unique_docs), key=lambda x: x[0], reverse=True)
        return [doc for score, doc in doc_scores[:5]]

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # âœ¨ [2ë‹¨ê³„] ì‚¬ìš©ì ê´€ì‹¬ ë¶„ì•¼(interests)ì— ë”°ë¼ ë™ì ìœ¼ë¡œ Retrieverë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    def get_dynamic_retriever(inputs: dict):
        interests = inputs.get("interests", [])
        search_kwargs = {'k': 20}
        
        # ê´€ì‹¬ ë¶„ì•¼ê°€ ì„¤ì •ëœ ê²½ìš°, ë©”íƒ€ë°ì´í„° í•„í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        if interests:
            # QdrantëŠ” $or ì¡°ê±´ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, should ì¡°ê±´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            # í•˜ì§€ë§Œ ê°„ë‹¨í•œ êµ¬í˜„ì„ ìœ„í•´ ì—¬ê¸°ì„œëŠ” ì²«ë²ˆì§¸ ê´€ì‹¬ì‚¬ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
            # ê³ ê¸‰ í•„í„°ë§ì€ Qdrantì˜ í•„í„°ë§ ë¬¸ë²•ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ 'must' ì¡°ê±´ìœ¼ë¡œ ì²«ë²ˆì§¸ ê´€ì‹¬ì‚¬ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
            # ì‹¤ì œë¡œëŠ” ì—¬ëŸ¬ ê´€ì‹¬ì‚¬ì— ëŒ€í•´ 'should' ì¡°ê±´ì„ êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
            search_kwargs['filter'] = {
                "must": [{"key": "metadata.policy_type", "match": {"any": interests}}]
            }
            
        base_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)
        
        multi_query_retriever = MultiQueryRetriever.from_llm(
            retriever=base_retriever, llm=llm, prompt=query_prompt
        )
        return multi_query_retriever

    # RAG ì²´ì¸ êµ¬ì„±
    # 1. get_dynamic_retrieverë¥¼ í˜¸ì¶œí•˜ì—¬ í•„í„°ë§ëœ retrieverë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    # 2. í•´ë‹¹ retrieverë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    # 3. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ rerankí•˜ê³  í¬ë§·íŒ…í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    rag_chain = (
        {
            "documents": RunnableLambda(lambda inputs: get_dynamic_retriever(inputs).invoke(inputs["question"], config=RunnableConfig(run_name="retrieval"))),
            "question": lambda x: x["question"]
        }
        | RunnableLambda(rerank_documents).with_config(run_name="reranking")
        | {
            "answer": (
                RunnablePassthrough.assign(context=lambda docs: format_docs(docs))
                | response_prompt
                | llm
                | StrOutputParser()
            ),
            "sources": lambda docs: docs
        }
    )
    return rag_chain

# --- ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ---
try:
    llm, reranker_model, query_prompt, response_prompt_template = get_models_and_prompts()
    vectorstore = create_vector_store()
    rag_chain_with_source = setup_rag_chain(vectorstore, reranker_model, llm, response_prompt_template, query_prompt)
except Exception as e:
    st.error(f"RAG êµ¬ì„± ìš”ì†Œ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.exception(e)
    st.stop()

# -----------------------
# SIDEBAR UI
# -----------------------
if "messages" not in st.session_state:
    st.session_state.messages = []
if "profile" not in st.session_state:
    st.session_state.profile = {}
if "selected_question" not in st.session_state:
    st.session_state.selected_question = None

with st.sidebar:
    st.header("ğŸ¯ ë‚˜ì˜ ë§ì¶¤ ì¡°ê±´ ì„¤ì •")
    st.markdown("AIê°€ ë” ì •í™•í•œ ì •ì±…ì„ ì¶”ì²œí•˜ë„ë¡ ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    age = st.number_input("ë‚˜ì´(ë§Œ)", min_value=19, max_value=39, value=st.session_state.profile.get("age", 25))
    interests = st.multiselect(
        "ì£¼ìš” ê´€ì‹¬ ë¶„ì•¼",
        ['ì£¼ê±°', 'ì¼ìë¦¬/ì°½ì—…', 'ê¸ˆìœµ/ìì‚°', 'ë³µì§€/ë¬¸í™”'],
        default=st.session_state.profile.get("interests", [])
    )
    if st.button("âœ… ì¡°ê±´ ì €ì¥ ë° ë°˜ì˜", type="primary", use_container_width=True):
        st.session_state.profile = { "age": age, "interests": interests }
        st.success("ë§ì¶¤ ì¡°ê±´ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

        if interests:
            welcome_message = f"ì•ˆë…•í•˜ì„¸ìš”! {age}ì„¸, '{', '.join(interests)}' ë¶„ì•¼ì— ê´€ì‹¬ì´ ìˆìœ¼ì‹œêµ°ìš”. ì´ì œë¶€í„° ê´€ë ¨ ì •ì±… ìœ„ì£¼ë¡œ ì°¾ì•„ë“œë¦´ê²Œìš”!"
        else:
            welcome_message = f"ì•ˆë…•í•˜ì„¸ìš”! {age}ì„¸ì´ì‹œêµ°ìš”. ê´€ì‹¬ ë¶„ì•¼ë¥¼ ì„ íƒí•˜ì‹œë©´ ë” ì •í™•í•œ ì¶”ì²œì„ ë°›ì„ ìˆ˜ ìˆì–´ìš”."
        
        st.session_state.messages.append({"role": "assistant", "content": welcome_message})
        time.sleep(1)
        st.rerun()

# -----------------------
# MAIN UI
# -----------------------
st.title("ğŸ¤– ì²­ë…„ ì •ì±… íë ˆì´í„° v3")
st.caption("ê´€ì‹¬ ë¶„ì•¼ í•„í„°ë§ ê¸°ëŠ¥ì´ ì ìš©ëœ ë§ì¶¤í˜• íƒìƒ‰ê¸°")

recommended_questions_db = {
    "ì£¼ê±°": ["ì „ì„¸ë³´ì¦ê¸ˆ ì´ì ì§€ì› ì •ì±… ì•Œë ¤ì¤˜", "ì—­ì„¸ê¶Œ ì²­ë…„ì£¼íƒ ì‹ ì²­ ìê²©ì€?"],
    "ì¼ìë¦¬/ì°½ì—…": ["ì·¨ì—… ì¤€ë¹„ìƒì¸ë° ë©´ì ‘ ì •ì¥ ë¹Œë¦´ ìˆ˜ ìˆì–´?", "ì„œìš¸ì‹œì—ì„œ ì¸í„´ì‹­ í•  ìˆ˜ ìˆëŠ” í”„ë¡œê·¸ë¨ ì°¾ì•„ì¤˜"],
    "ê¸ˆìœµ/ìì‚°": ["í¬ë§ë‘ë°° ì²­ë…„í†µì¥ ê°€ì… ì¡°ê±´ì´ ë­ì•¼?", "í•™ìê¸ˆ ëŒ€ì¶œ ì´ì ì§€ì› ì‚¬ì—…ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜"],
    "ë³µì§€/ë¬¸í™”": ["ì„œìš¸ì‹œ ì²­ë…„ìˆ˜ë‹¹ ì‹ ì²­ ë°©ë²• ì•Œë ¤ì¤˜", "ì²­ë…„ë“¤ì´ ë¬¸í™”ìƒí™œ ì¦ê¸¸ ìˆ˜ ìˆê²Œ ì§€ì›í•´ì£¼ëŠ” ì •ì±… ìˆì–´?"]
}

st.markdown("##### ğŸ‘‡ ì´ëŸ° ì§ˆë¬¸ì€ ì–´ë– ì„¸ìš”?")
profile_interests = st.session_state.get("profile", {}).get("interests", [])
if profile_interests:
    questions_to_show = recommended_questions_db.get(profile_interests[0], [])
else:
    questions_to_show = ["ì „ì„¸ë³´ì¦ê¸ˆ ì´ì ì§€ì› ì •ì±… ì•Œë ¤ì¤˜", "ì·¨ì—… ì¤€ë¹„ìƒì¸ë° ë©´ì ‘ ì •ì¥ ë¹Œë¦´ ìˆ˜ ìˆì–´?"]

cols = st.columns(len(questions_to_show))
for i, question in enumerate(questions_to_show):
    if cols[i].button(question, use_container_width=True, key=f"rec_q_{i}"):
        st.session_state.selected_question = question
        st.rerun()

# -----------------------
# CHAT UI
# -----------------------
if not st.session_state.messages:
    profile = st.session_state.get("profile", {})
    if profile.get("age") and profile.get("interests"):
         welcome_message = f"ì•ˆë…•í•˜ì„¸ìš”! {profile['age']}ì„¸, '{', '.join(profile['interests'])}' ë¶„ì•¼ì— ê´€ì‹¬ì´ ìˆìœ¼ì‹œêµ°ìš”. ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"
    else:
        welcome_message = "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–¤ ì •ì±…ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë§ì¶¤ ì •ë³´ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    st.session_state.messages.append({"role": "assistant", "content": welcome_message})

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message and message["sources"]:
            with st.expander("ğŸ“š ê·¼ê±° ìë£Œ í™•ì¸í•˜ê¸°"):
                for source in message["sources"]:
                    st.info(f"ì¶œì²˜: {source.metadata.get('source', 'N/A')} (í˜ì´ì§€: {source.metadata.get('page', 'N/A')}) | ìœ í˜•: {source.metadata.get('policy_type', 'N/A')}")
                    st.write(source.page_content)

prompt = st.chat_input("ê¶ê¸ˆí•œ ì •ì±…ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
if st.session_state.selected_question:
    prompt = st.session_state.selected_question
    st.session_state.selected_question = None

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("AIê°€ ë§ì¶¤ ì •ì±… ì •ë³´ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤..."):
            try:
                # âœ¨ [3ë‹¨ê³„] RAG ì²´ì¸ í˜¸ì¶œ ì‹œ ì‚¬ìš©ì ê´€ì‹¬ ë¶„ì•¼ ì „ë‹¬
                profile_interests = st.session_state.get("profile", {}).get("interests", [])
                
                result = rag_chain_with_source.invoke({
                    "question": prompt,
                    "interests": profile_interests
                })
                
                response = result.get("answer", "ì˜¤ë¥˜: ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                final_docs = result.get("sources", [])

                if not final_docs:
                     response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì„ íƒí•˜ì‹  ê´€ì‹¬ ë¶„ì•¼ì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë¶„ì•¼ë¥¼ ì„ íƒí•˜ì‹œê±°ë‚˜ ì§ˆë¬¸ì„ ë°”ê¿”ë³´ì„¸ìš”."

            except Exception as e:
                response = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                final_docs = []
                st.error(response)
                st.exception(e)

        st.markdown(response)
        st.session_state.messages.append({
            "role": "assistant",
            "content": response,
            "sources": final_docs
        })

    st.rerun()
