import streamlit as st
import time
import os
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Qdrant
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from sentence_transformers import CrossEncoder

# -----------------------
# PAGE CONFIG
# -----------------------

st.set_page_config(
    page_title="ì •ì±… íë ˆì´í„°",
    page_icon="ğŸ¤–",
    layout="wide",
)

# -----------------------
# SECRETS
# -----------------------

# Streamlit Cloudì˜ Secretsì—ì„œ API í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•©ë‹ˆë‹¤.
try:
    openai_api_key = st.secrets["OPENAI_API_KEY"]
except Exception:
    st.error("ì˜¤ë¥˜: OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Streamlit Cloudì˜ Secrets ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()


# -----------------------
# CSS
# -----------------------

st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+KR:wght@300;400;500;600;700&display=swap');
html, body, [class*="css"]  { font-family: 'IBM Plex Sans KR', sans-serif; }
.stApp { background-color: #F0F2F6; }
[data-testid="stSidebar"] { background-color: #FFFFFF; border-right: 1px solid #E0E0E0; }
.stButton > button {
    border: 1px solid #E0E0E0; border-radius: 10px; color: #31333F;
    background-color: #FFFFFF; transition: all 0.2s ease-in-out;
    box-shadow: 0 1px 3px rgba(0,0,0,0.05);
}
.stButton > button:hover {
    border-color: #0068C9; color: #0068C9;
    box-shadow: 0 2px 5px rgba(0,0,0,0.1);
}
div[data-testid="stSidebar"] .stButton > button {
    background-color: #0068C9; color: white; border: none;
}
div[data-testid="stSidebar"] .stButton > button:hover {
    background-color: #0055A3; color: white;
}
</style>
""", unsafe_allow_html=True)

# -----------------------
# RAG COMPONENTS
# -----------------------

DATA_PATH = "./data"

@st.cache_resource
def get_rag_components():
    """
    RAG íŒŒì´í”„ë¼ì¸ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œë“¤ì„ ì„¤ì •í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ì•± ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ í˜¸ì¶œë˜ë©°, ê²°ê³¼ëŠ” ìºì‹œì— ì €ì¥ë©ë‹ˆë‹¤.
    """
    st.sidebar.info("ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    if not os.path.exists(DATA_PATH) or not any(f.endswith('.pdf') for f in os.listdir(DATA_PATH)):
        st.error(f"ì˜¤ë¥˜: '{DATA_PATH}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    documents = []
    for file in os.listdir(DATA_PATH):
        if file.endswith('.pdf'):
            loader = PyPDFLoader(os.path.join(DATA_PATH, file))
            documents.extend(loader.load())

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)
    chunks = text_splitter.split_documents(documents)

    st.sidebar.info("ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    embeddings = OpenAIEmbeddings(api_key=openai_api_key)
    vectorstore = Qdrant.from_documents(
        chunks, embeddings, location=":memory:", collection_name="policy_documents"
    )
    retriever = vectorstore.as_retriever(search_kwargs={'k': 20})
    st.sidebar.success("ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")

    llm = ChatOpenAI(api_key=openai_api_key, model="gpt-4o-mini", temperature=0.1)

    st.sidebar.info("Re-ranker ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    reranker_model = CrossEncoder('bongsoo/kpf-cross-encoder-v1')
    st.sidebar.success("ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì™„ë£Œ!")

    return retriever, llm, reranker_model

try:
    retriever, llm, reranker_model = get_rag_components()
except Exception as e:
    st.error(f"RAG êµ¬ì„± ìš”ì†Œ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.stop()

# -----------------------
# CONVERSATIONAL RAG CHAIN
# -----------------------

contextualize_q_system_prompt = """
Given a chat history and the latest user question which might reference context in the chat history, 
formulate a standalone question which can be understood without the chat history. 
Do NOT answer the question, just reformulate it if needed and otherwise return it as is.
"""
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)

qa_system_prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì •ë¶€ ì •ì±… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ì˜ 'ë¬¸ì„œ ë‚´ìš©'ì„ ë°”íƒ•ìœ¼ë¡œ, ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì— ì—†ëŠ” ì •ë³´ëŠ” ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
\n\n
[ë¬¸ì„œ ë‚´ìš©]
{context}
"""
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# -----------------------
# SESSION STATE & SIDEBAR
# -----------------------

if "messages" not in st.session_state:
    st.session_state.messages = []
# [ìˆ˜ì •] profile ì„¸ì…˜ ìƒíƒœë¥¼ ì•± ì‹œì‘ ì‹œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
if "profile" not in st.session_state:
    st.session_state.profile = {}

from langchain_core.messages import AIMessage, HumanMessage
def get_chat_history(messages):
    history = []
    for msg in messages:
        if msg["role"] == "user":
            history.append(HumanMessage(content=msg["content"]))
        else:
            history.append(AIMessage(content=msg["content"]))
    return history

conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: get_chat_history(st.session_state.messages),
    input_messages_key="input",
    history_messages_key="chat_history",
    output_messages_key="answer",
)

with st.sidebar:
    st.header("ğŸ¯ ë‚˜ì˜ ë§ì¶¤ ì¡°ê±´ ì„¤ì •")
    st.markdown("AIê°€ ë” ì •í™•í•œ ì •ì±…ì„ ì¶”ì²œí•˜ë„ë¡ ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    # ì´ì œ st.session_state.profileì´ í•­ìƒ ì¡´ì¬í•˜ë¯€ë¡œ .get() í˜¸ì¶œì´ ì•ˆì „í•©ë‹ˆë‹¤.
    age = st.number_input("ë‚˜ì´(ë§Œ)", min_value=18, max_value=100, value=st.session_state.profile.get("age", 25))
    interests = st.multiselect(
        "ì£¼ìš” ê´€ì‹¬ ë¶„ì•¼",
        ['ì£¼ê±°', 'ì¼ìë¦¬/ì°½ì—…', 'ê¸ˆìœµ/ìì‚°', 'ë³µì§€/ë¬¸í™”'],
        default=st.session_state.profile.get("interests", [])
    )
    if st.button("âœ… ì¡°ê±´ ì €ì¥ ë° ë°˜ì˜", type="primary", use_container_width=True):
        st.session_state.profile = { "age": age, "interests": interests }
        st.success("ë§ì¶¤ ì¡°ê±´ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        time.sleep(1)
        st.rerun()


# -----------------------
# MAIN UI & CHAT LOGIC
# -----------------------

st.title("ğŸ¤– ì²­ë…„ ì •ì±… íë ˆì´í„°")
st.caption("AI ê¸°ë°˜ ë§ì¶¤í˜• ì •ì±… íƒìƒ‰ê¸°")

# ì´ˆê¸° í™˜ì˜ ë©”ì‹œì§€
if not st.session_state.messages:
    profile = st.session_state.get("profile", {})
    if profile.get("age") and profile.get("interests"):
        welcome_message = f"ì•ˆë…•í•˜ì„¸ìš”! {profile['age']}ì„¸, '{profile['interests'][0]}' ë¶„ì•¼ì— ê´€ì‹¬ì´ ìˆìœ¼ì‹œêµ°ìš”."
    else:
        welcome_message = "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–¤ ì •ì±…ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"
    st.session_state.messages.append({"role": "assistant", "content": welcome_message})

# ì´ì „ ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ê¶ê¸ˆí•œ ì •ì±…ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        try:
            response_placeholder = st.empty()
            full_response = ""

            stream_handler = conversational_rag_chain.stream(
                {"input": prompt},
                {"configurable": {"session_id": "any_session_id"}},
            )

            for chunk in stream_handler:
                if "answer" in chunk:
                    full_response += chunk["answer"]
                    response_placeholder.markdown(full_response + "â–Œ")
            
            response_placeholder.markdown(full_response)
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})

        except Exception as e:
            error_message = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            st.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": error_message})
            
    st.rerun()
