# ======================================================================
# íŒŒì¼ 1: app.py
# ì§€ëŠ¥í˜• ì¿¼ë¦¬ í™•ì¥ ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ê³ , RAG íŒŒì´í”„ë¼ì¸ ë¡œì§ì´ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.
# ======================================================================
import streamlit as st
import time
import os
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Qdrant
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from sentence_transformers import CrossEncoder

# -----------------------
# PAGE CONFIG
# -----------------------

st.set_page_config(
    page_title="ì •ì±… íë ˆì´í„°",
    page_icon="ğŸ¤–",
    layout="wide",
)

# -----------------------
# SECRETS
# -----------------------

try:
    openai_api_key = st.secrets["OPENAI_API_KEY"]
except Exception:
    st.error("ì˜¤ë¥˜: OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Streamlit Cloudì˜ Secrets ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# -----------------------
# CSS
# -----------------------

st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+KR:wght@300;400;500;600;700&display=swap');
html, body, [class*="css"]  { font-family: 'IBM Plex Sans KR', sans-serif; }
.stApp { background-color: #F0F2F6; }
[data-testid="stSidebar"] { background-color: #FFFFFF; border-right: 1px solid #E0E0E0; }
.stButton > button {
    border: 1px solid #E0E0E0; border-radius: 10px; color: #31333F;
    background-color: #FFFFFF; transition: all 0.2s ease-in-out;
    box-shadow: 0 1px 3px rgba(0,0,0,0.05);
}
.stButton > button:hover {
    border-color: #0068C9; color: #0068C9;
    box-shadow: 0 2px 5px rgba(0,0,0,0.1);
}
div[data-testid="stSidebar"] .stButton > button {
    background-color: #0068C9; color: white; border: none;
}
div[data-testid="stSidebar"] .stButton > button:hover {
    background-color: #0055A3; color: white;
}
</style>
""", unsafe_allow_html=True)

# -----------------------
# RAG COMPONENTS
# -----------------------

DATA_PATH = "./data"

@st.cache_resource
def get_rag_components():
    st.sidebar.info("ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    if not os.path.exists(DATA_PATH) or not any(f.endswith('.pdf') for f in os.listdir(DATA_PATH)):
        st.error(f"ì˜¤ë¥˜: '{DATA_PATH}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    documents = []
    for file in os.listdir(DATA_PATH):
        if file.endswith('.pdf'):
            loader = PyPDFLoader(os.path.join(DATA_PATH, file))
            documents.extend(loader.load())

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150
    )
    chunks = text_splitter.split_documents(documents)

    st.sidebar.info("ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    embeddings = OpenAIEmbeddings(api_key=openai_api_key)
    vectorstore = Qdrant.from_documents(
        chunks, embeddings, location=":memory:", collection_name="policy_documents"
    )
    retriever = vectorstore.as_retriever(search_kwargs={'k': 20})
    st.sidebar.success("ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")

    llm = ChatOpenAI(api_key=openai_api_key, model="gpt-4o-mini", temperature=0.1)

    prompt_template = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì •ë¶€ ì •ì±… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ì˜ 'ë¬¸ì„œ ë‚´ìš©'ì„ ë°”íƒ•ìœ¼ë¡œ, ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì— ì—†ëŠ” ì •ë³´ëŠ” ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        [ë¬¸ì„œ ë‚´ìš©]
        {context}
        [ì‚¬ìš©ì ì§ˆë¬¸]
        {question}
        [ë‹µë³€]
        """
    )

    st.sidebar.info("Re-ranker ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    reranker_model = CrossEncoder('bongsoo/kpf-cross-encoder-v1')
    st.sidebar.success("ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì™„ë£Œ!")

    return retriever, llm, prompt_template, reranker_model

try:
    retriever, llm, prompt_template, reranker_model = get_rag_components()
except Exception as e:
    st.error(f"RAG êµ¬ì„± ìš”ì†Œ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.stop()

# -----------------------
# KEYWORD DICT
# -----------------------

keyword_dict = {
    "ì²­ë…„ ì›”ì„¸": ["ì›”ì„¸ ì§€ì›", "ì„ëŒ€ë£Œ ì§€ì›", "ì²­ë…„ ì£¼ê±°"],
    "ì²­ë…„ìˆ˜ë‹¹": ["ì²­ë…„ ì§€ì›ê¸ˆ", "ì„œìš¸ì‹œ ì²­ë…„ìˆ˜ë‹¹", "ìƒí™œë¹„ ì§€ì›"],
    "ì²­ë…„AIì·¨ì—…ìº í”„": ["AIì·¨ì—…ìº í”„", "ì²­ë…„ ì·¨ì—… í”„ë¡œê·¸ë¨"],
    "ì²­ë…„ë„ì „": ["ì²­ë…„ë„ì „ ì§€ì›ì‚¬ì—…", "ì‹¬ë¦¬ ì§€ì› í”„ë¡œê·¸ë¨"],
    "ì—­ì„¸ê¶Œ ì²­ë…„ì£¼íƒ": ["ì²­ë…„ ì£¼ê±°", "ì—­ì„¸ê¶Œ ì£¼íƒ"],
    "ì„œìš¸í˜• ì²­ë…„ì¸í„´ ì§ë¬´ìº í”„": ["ì²­ë…„ ì¸í„´ì‹­", "ì²­ë…„ ì§ë¬´ ì²´í—˜", "ì„œìš¸ ì²­ë…„ ì¸í„´"],
}

def expand_keywords(user_query):
    expanded = [user_query]
    for key, synonyms in keyword_dict.items():
        if key in user_query:
            expanded += synonyms
    return list(set(expanded))

# -----------------------
# SIDEBAR UI
# -----------------------

if "messages" not in st.session_state:
    st.session_state.messages = []
if "profile" not in st.session_state:
    st.session_state.profile = {}
if "selected_question" not in st.session_state:
    st.session_state.selected_question = None

with st.sidebar:
    st.header("ğŸ¯ ë‚˜ì˜ ë§ì¶¤ ì¡°ê±´ ì„¤ì •")
    st.markdown("AIê°€ ë” ì •í™•í•œ ì •ì±…ì„ ì¶”ì²œí•˜ë„ë¡ ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    age = st.number_input("ë‚˜ì´(ë§Œ)", min_value=18, max_value=100, value=st.session_state.profile.get("age", 25))
    interests = st.multiselect(
        "ì£¼ìš” ê´€ì‹¬ ë¶„ì•¼",
        ['ì£¼ê±°', 'ì¼ìë¦¬/ì°½ì—…', 'ê¸ˆìœµ/ìì‚°', 'ë³µì§€/ë¬¸í™”'],
        default=st.session_state.profile.get("interests", [])
    )
    if st.button("âœ… ì¡°ê±´ ì €ì¥ ë° ë°˜ì˜", type="primary", use_container_width=True):
        st.session_state.profile = { "age": age, "interests": interests }
        st.success("ë§ì¶¤ ì¡°ê±´ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        time.sleep(1)
        st.rerun()

# -----------------------
# MAIN UI
# -----------------------

st.title("ğŸ¤– ì²­ë…„ ì •ì±… íë ˆì´í„°")
st.caption("AI ê¸°ë°˜ ë§ì¶¤í˜• ì •ì±… íƒìƒ‰ê¸°")

recommended_questions_db = {
    "ì£¼ê±°": ["ì„ì°¨ë³´ì¦ê¸ˆ ì•Œë ¤ì¤˜", "ì—­ì„¸ê¶Œ ì²­ë…„ì£¼íƒ ì•Œë ¤ì¤˜?"],
    "ì¼ìë¦¬/ì°½ì—…": ["ì²­ë…„ë‚´ì¼ì±„ì›€ê³µì œ ì§€ê¸ˆë„ ì‹ ì²­í•  ìˆ˜ ìˆë‚˜?", "ì„œìš¸ì‹œ ì²­ë…„ìˆ˜ë‹¹ìœ¼ë¡œ ë¬´ì—‡ì— ì“¸ ìˆ˜ ìˆëŠ”ì§€ ì•Œë ¤ì¤˜"],
    "ê¸ˆìœµ/ìì‚°": ["í¬ë§ë‘ë°° ì²­ë…„í†µì¥ì€ ì–´ë–¤ í˜œíƒì´ ìˆë‚˜?", "ì²­ë…„ë‚´ì¼ì €ì¶•ê³„ì¢Œ ì§€ì›ê¸ˆì€ ì–¼ë§ˆê¹Œì§€ ë°›ì„ ìˆ˜ ìˆë‚˜?"],
    "ë³µì§€/ë¬¸í™”": ["ê³ ë¦½ì€ë‘”ì²­ë…„ ì§€ì›ì‚¬ì—…ì€ ì–´ë–¤ ë‚´ìš©ì´ì•¼?", "ìë¦½ì¤€ë¹„ì²­ë…„ ìë¦½ìˆ˜ë‹¹ì€ ì–´ë–»ê²Œ ì‹ ì²­í•˜ë‚˜?"]
}

st.markdown("##### ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
profile_interests = st.session_state.get("profile", {}).get("interests", [])
questions_to_show = recommended_questions_db.get(
    profile_interests[0],
    ["ìë¦½ì¤€ë¹„ì²­ë…„ ìë¦½ìˆ˜ë‹¹ ì•Œë ¤ì¤˜?", "í¬ë§ë‘ë°° ì²­ë…„í†µì¥ ì‹ ì²­ ì¡°ê±´ ì•Œë ¤ì¤˜"]
) if profile_interests else ["ìë¦½ì¤€ë¹„ì²­ë…„ ìë¦½ìˆ˜ë‹¹ ì•Œë ¤ì¤˜?", "í¬ë§ë‘ë°° ì²­ë…„í†µì¥ ì‹ ì²­ ì¡°ê±´ ì•Œë ¤ì¤˜"]

cols = st.columns(len(questions_to_show))
for i, question in enumerate(questions_to_show):
    if cols[i].button(question, use_container_width=True, key=f"rec_q_{i}"):
        st.session_state.selected_question = question

# -----------------------
# CHAT UI
# -----------------------

if not st.session_state.messages:
    profile = st.session_state.get("profile", {})
    welcome_message = f"ì•ˆë…•í•˜ì„¸ìš”! {profile['age']}ì„¸, '{profile['interests'][0]}' ë¶„ì•¼ì— ê´€ì‹¬ì´ ìˆìœ¼ì‹œêµ°ìš”." if profile.get("age") and profile.get("interests") else "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–¤ ì •ì±…ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"
    st.session_state.messages.append({"role": "assistant", "content": welcome_message})

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message:
            with st.expander("ğŸ“š ê·¼ê±° ìë£Œ í™•ì¸í•˜ê¸°"):
                for source in message["sources"]:
                    st.info(f"ì¶œì²˜: {source.metadata.get('source', 'N/A')} (í˜ì´ì§€: {source.metadata.get('page', 'N/A')})")
                    st.write(source.page_content)

prompt = st.chat_input("ê¶ê¸ˆí•œ ì •ì±…ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
if st.session_state.selected_question:
    prompt = st.session_state.selected_question
    st.session_state.selected_question = None

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                # --- [ê°œì„ ] 1. ì´ˆê¸° ê²€ìƒ‰ ë° ê´€ë ¨ì„± í‰ê°€ ---
                initial_docs = retriever.invoke(prompt)
                unique_initial_docs = list({doc.page_content: doc for doc in initial_docs}.values())
                
                relevance_score = 0.0
                final_docs = []

                if unique_initial_docs:
                    pairs = [[prompt, doc.page_content] for doc in unique_initial_docs]
                    scores = reranker_model.predict(pairs)
                    if scores.any():
                        relevance_score = max(scores)
                
                RELEVANCE_THRESHOLD = 0.1 # ê´€ë ¨ì„± ì„ê³„ê°’ ì„¤ì •

                # --- [ê°œì„ ] 2. ì¡°ê±´ë¶€ ì¿¼ë¦¬ í™•ì¥ ì‹¤í–‰ ---
                if relevance_score < RELEVANCE_THRESHOLD:
                    with st.spinner("ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ê°€ ë‚®ì•„, ì¶”ê°€ì ì¸ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤..."):
                        # í‚¤ì›Œë“œ ê¸°ë°˜ í™•ì¥
                        expanded_queries = expand_keywords(prompt)

                        # LLM ê¸°ë°˜ í™•ì¥
                        expansion_prompt = PromptTemplate.from_template(
                            """ë‹¹ì‹ ì€ í•œêµ­ ì²­ë…„ ì •ì±… ê²€ìƒ‰ì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                            ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ê´€ë ¨ì„±ì´ ë†’ì€ ì •ì±…ëª…, ì œë„ëª…, í˜¹ì€ í”„ë¡œê·¸ë¨ëª…ì„ ìµœëŒ€ 3ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.
                            ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ì§ˆë¬¸: {question}"""
                        )
                        query_expansion_chain = expansion_prompt | llm | StrOutputParser()
                        expanded_queries_str = query_expansion_chain.invoke({"question": prompt})
                        expanded_queries += [q.strip() for q in expanded_queries_str.split(',') if q.strip()]
                        expanded_queries = list(set(expanded_queries))

                        # í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ì¬ê²€ìƒ‰
                        all_retrieved_docs = []
                        for q in expanded_queries:
                            all_retrieved_docs.extend(retriever.invoke(q))
                        
                        unique_docs = list({doc.page_content: doc for doc in all_retrieved_docs}.values())
                        
                        # ìµœì¢… ì¬ìˆœìœ„í™”
                        if unique_docs:
                            pairs = [[prompt, doc.page_content] for doc in unique_docs]
                            scores = reranker_model.predict(pairs)
                            doc_scores = sorted(zip(scores, unique_docs), key=lambda x: x[0], reverse=True)
                            final_docs = [doc for score, doc in doc_scores[:3]]
                else:
                    # ê´€ë ¨ì„±ì´ ë†’ìœ¼ë©´ ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©
                    doc_scores = sorted(zip(scores, unique_initial_docs), key=lambda x: x[0], reverse=True)
                    final_docs = [doc for score, doc in doc_scores[:3]]

                # --- 3. ìµœì¢… ë‹µë³€ ìƒì„± ---
                if final_docs:
                    context = "\n\n".join(doc.page_content for doc in final_docs)
                    final_prompt = prompt_template.format(context=context, question=prompt)
                    response = llm.invoke(final_prompt).content
                else:
                    response = "ì£„ì†¡í•©ë‹ˆë‹¤. PDF ë¬¸ì„œì—ì„œë„ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”!"

                st.markdown(response)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response,
                    "sources": final_docs
                })

            except Exception as e:
                error_message = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                st.error(error_message)
                st.session_state.messages.append({"role": "assistant", "content": error_message})

    st.rerun()
