# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lip1hlxbZSkbBUycIRkLGtHBN5sF7U93
"""

import streamlit as st
import os
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# --- 1. 데이터베이스 준비 및 RAG 파이프라인 설정 ---

# ChromaDB 저장 경로
DB_DIR = "./chroma_db"

def setup_database():
    """
    ChromaDB가 존재하는지 확인하고, 없으면 새로 생성하는 함수.
    Streamlit Cloud의 임시 저장소에 DB를 구축합니다.
    """
    if os.path.exists(DB_DIR):
        st.sidebar.info("기존 데이터베이스를 로드했습니다.")
        return

    st.sidebar.info("새로운 데이터베이스를 구축하고 있습니다. 잠시만 기다려주세요...")

    # 1. 문서 로드
    docs = []
    data_path = "./data"
    pdf_files = [f for f in os.listdir(data_path) if f.endswith('.pdf')]
    if not pdf_files:
        st.error("오류: data 폴더에 PDF 파일이 없습니다.")
        st.stop()

    for pdf_file in pdf_files:
        loader = PyPDFLoader(os.path.join(data_path, pdf_file))
        docs.extend(loader.load())

    # 2. 텍스트 분할
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_documents(docs)

    # 3. 임베딩 및 ChromaDB 저장
    try:
        # OpenAI API 키는 Streamlit Secrets에서 자동으로 로드됩니다.
        embeddings = OpenAIEmbeddings()
        Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory=DB_DIR
        )
        st.sidebar.success("데이터베이스 구축이 완료되었습니다!")
    except Exception as e:
        st.error(f"데이터베이스 구축 중 오류가 발생했습니다: {e}")
        st.stop()

@st.cache_resource
def get_rag_chain():
    """
    RAG 체인을 초기화하고 반환하는 함수.
    Streamlit의 캐싱을 사용하여 리소스를 효율적으로 관리합니다.
    """
    try:
        # OpenAI API 키는 Streamlit Secrets에서 자동으로 로드됩니다.
        embeddings = OpenAIEmbeddings()

        # 디스크에 저장된 ChromaDB를 로드
        vector_store = Chroma(
            persist_directory=DB_DIR,
            embedding_function=embeddings
        )
        retriever = vector_store.as_retriever()

        # 언어 모델(LLM) 초기화
        llm = ChatOpenAI(model="gpt-4o", temperature=0)

        # 프롬프트 템플릿 정의
        template = """당신은 대한민국 정부 정책 전문가입니다. 사용자의 질문에 대해 아래의 '문서 내용'을 바탕으로, 명확하고 친절하게 답변해주세요.
        문서 내용에 없는 정보는 답변에 포함하지 마세요. 답변은 항상 한국어로 작성해주세요.

        [문서 내용]
        {context}

        [사용자 질문]
        {question}

        [답변]
        """

        prompt = PromptTemplate.from_template(template)

        # RAG 체인 구성
        rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        return rag_chain, retriever
    except Exception as e:
        st.error(f"RAG 파이프라인 초기화 중 오류가 발생했습니다. OpenAI API 키가 올바르게 설정되었는지 확인하세요. 오류: {e}")
        st.stop()

# --- 2. Streamlit UI 구성 ---

st.set_page_config(page_title="정책 큐레이터", page_icon="🤖")
st.title("🤖 정책 큐레이터")
st.caption("AI가 정부 정책 문서를 분석하여 질문에 답변해 드립니다.")

with st.sidebar:
    st.header("안내")
    st.markdown("""
    이 앱은 RAG(검색 증강 생성) 기술을 사용하여 PDF 문서의 내용을 기반으로 질문에 답변합니다.

    **배포 안내:**
    1. 이 프로젝트를 GitHub에 업로드하세요.
    2. Streamlit Cloud에 로그인하여 새 앱을 만듭니다.
    3. GitHub 리포지토리를 연결합니다.
    4. **Advanced settings...** 에서 `OPENAI_API_KEY`를 Secrets에 추가합니다.
       - 형식: `OPENAI_API_KEY = "sk-..."`
    5. **Deploy!** 버튼을 누르면 배포가 시작됩니다.

    첫 배포 시에는 PDF를 처리하여 데이터베이스를 구축하므로 시간이 다소 걸릴 수 있습니다.
    """)

# 데이터베이스 설정 실행
setup_database()

# RAG 체인과 retriever 로드
rag_chain, retriever = get_rag_chain()

# 세션 상태를 사용하여 대화 기록 저장
if "messages" not in st.session_state:
    st.session_state.messages = []

# 이전 대화 기록 표시
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        # 근거 자료가 있는 경우, 확장 가능한 형태로 표시
        if "sources" in message and message["sources"]:
            with st.expander("📚 근거 자료 확인하기"):
                for i, source in enumerate(message["sources"]):
                    st.markdown(f"**문서 {i+1} (출처: {source.metadata.get('source', 'N/A')}, 페이지: {source.metadata.get('page', 'N/A')})**")
                    st.markdown(f"> {source.page_content}")
                    st.markdown("---")

# 사용자 입력 처리
if prompt := st.chat_input("궁금한 정책에 대해 질문해보세요."):
    # 사용자 메시지를 대화 기록에 추가하고 화면에 표시
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 로딩 스피너 표시
    with st.spinner("답변을 생성하는 중입니다..."):
        try:
            # RAG 체인을 직접 호출하여 답변 생성
            answer = rag_chain.invoke(prompt)

            # 답변의 근거가 된 문서 검색
            sources = retriever.invoke(prompt)

            # AI 응답을 대화 기록에 추가하고 화면에 표시
            bot_message = {"role": "assistant", "content": answer, "sources": sources}
            st.session_state.messages.append(bot_message)

            # Streamlit이 메시지를 다시 그리도록 강제
            st.rerun()

        except Exception as e:
            st.error(f"답변 생성 중 오류가 발생했습니다: {e}")